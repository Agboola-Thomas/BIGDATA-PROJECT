CREATE SOURCE CONNECTOR SOURCE_LOG_FILEPULSE WITH (
'connector.class' = 'io.streamthoughts.kafka.connect.filepulse.source.FilePulseSourceConnector',
'fs.cleanup.policy.class' = 'io.streamthoughts.kafka.connect.filepulse.fs.clean.LogCleanupPolicy',
'fs.cleanup.policy.triggered.on' = 'COMMITTED',
'fs.listing.class' = 'io.streamthoughts.kafka.connect.filepulse.fs.LocalFSDirectoryListing',
'fs.listing.directory.path' = '/logs-to-analyze/',
'fs.listing.filters' = 'io.streamthoughts.kafka.connect.filepulse.fs.filter.RegexFileListFilter',
'fs.listing.interval.ms' = '10000',
'file.filter.regex.pattern' = '.*\\.log$',
'offset.policy.class' = 'io.streamthoughts.kafka.connect.filepulse.offset.DefaultSourceOffsetPolicy',
'offset.attributes.string' = 'inode',
'read.max.wait.ms' = '30000',
'topic' = 'connect-file-pulse-raw-logs',
'tasks.reader.class' = 'io.streamthoughts.kafka.connect.filepulse.fs.reader.LocalRowFileInputReader',
'tasks.file.status.storage.class' = 'io.streamthoughts.kafka.connect.filepulse.state.KafkaFileObjectStateBackingStore',
'tasks.file.status.storage.bootstrap.servers' = 'broker:29092',
'tasks.file.status.storage.topic' = 'connect-file-pulse-status-raw-logs',
'tasks.file.status.storage.topic.partitions' = '10',
'tasks.file.status.storage.topic.replication.factor' = '1',
'tasks.max' = '1',
'name' = 'source-log-filepulse',
'tasks.empty.poll.wait.ms' = '500'
);